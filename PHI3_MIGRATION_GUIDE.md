# Phi-3 Migration Guide: Upgrading from TinyLlama

## üöÄ Overview

This guide walks you through migrating your Omeruta Brain system from TinyLlama to **Phi-3-mini-128k-instruct**, Microsoft's superior small language model optimized for RAG applications.

## üéØ Why Phi-3?

### Key Advantages Over TinyLlama

| Feature             | TinyLlama-1.1B  | Phi-3-mini-128k | Improvement                          |
| ------------------- | --------------- | --------------- | ------------------------------------ |
| **Context Window**  | 8K tokens       | 128K tokens     | **16x larger**                       |
| **RAG Performance** | Basic           | Excellent       | **Much better document integration** |
| **Apple Silicon**   | Limited         | Optimized       | **Native M1/M2 support**             |
| **JSON Generation** | Prone to errors | Highly reliable | **Zero hallucinated outputs**        |
| **Memory Usage**    | ~2GB            | ~3.8GB          | **Reasonable increase**              |
| **Inference Speed** | 20-30 tok/sec   | 25-35 tok/sec   | **Faster on M1**                     |

### RAG-Specific Benefits

1. **Extended Context Processing**: Can handle much longer retrieved documents without truncation
2. **Better Context Synthesis**: Maintains coherence across larger knowledge bases
3. **Improved Multi-turn Conversations**: Retains more conversation history
4. **Superior Instruction Following**: Better adherence to system prompts and formatting

## üîß Migration Process

### Option 1: Automated Migration (Recommended)

```bash
# Navigate to your project directory
cd omeruta_brain

# Run the migration script
python migration_to_phi3.py
```

The script will:

- ‚úÖ Check system requirements
- üì• Download Phi-3 model (~7GB)
- üß™ Test model functionality
- üîó Verify RAG integration
- üßπ Offer cleanup options

### Option 2: Manual Migration

#### Step 1: Update Dependencies

Ensure you have the required packages:

```bash
pip install torch transformers sentence-transformers
```

#### Step 2: Model Service Integration

The new `Phi3ModelService` is already integrated. Key changes:

- **New Service**: `omeruta_brain/agents/services/phi3_model_service.py`
- **Enhanced Agent**: `EnhancedPhi3Agent` (backward compatible as `EnhancedTinyLlamaAgent`)
- **Updated Settings**: Configuration optimized for Phi-3's capabilities

#### Step 3: Configuration Changes

Your `settings.py` has been updated with:

```python
AI_MODELS = {
    "LOCAL_MODELS": {
        "phi3": {
            "model_name": "microsoft/Phi-3-mini-128k-instruct",
            "enabled": True,
            "context_window": 128000,
            "max_tokens": 2048,
            "optimized_for": ["Apple Silicon", "RAG", "JSON Generation"],
        }
    },
    "DEFAULT_LOCAL_MODEL": "phi3",  # Changed from "tinyllama"
}

VECTOR_SETTINGS = {
    "MAX_CONTEXT_TOKENS": 25000,  # Increased from 2000
    "RERANK_TOP_K": 30,          # Increased from 20
    "FINAL_TOP_K": 10,           # Increased from 5
    "PHI3_OPTIMIZED": True,      # New optimization flag
}
```

## üß™ Testing Your Migration

### 1. Basic Model Test

```python
from agents.services.phi3_model_service import Phi3ModelService

service = Phi3ModelService()
if service.initialize_model():
    response = service.generate_response(
        prompt="Explain the benefits of Phi-3 for RAG systems",
        max_tokens=300
    )
    print(response)
```

### 2. RAG Integration Test

```python
from agents.services.enhanced_tinyllama_agent import EnhancedPhi3Agent

agent = EnhancedPhi3Agent(agent_type="general")
result = agent.process_message(
    message="What is artificial intelligence?",
    use_context=True,
    response_config={"max_tokens": 500}
)

print(f"Status: {result['status']}")
print(f"Used context: {result['used_context']}")
print(f"Sources: {len(result['sources'])}")
print(f"Response: {result['response'][:200]}...")
```

### 3. Performance Comparison

```python
# Test with longer context (Phi-3 advantage)
long_query = "Compare different approaches to machine learning and their applications in modern AI systems, including deep learning, reinforcement learning, and transfer learning."

result = agent.process_message(
    message=long_query,
    use_context=True,
    response_config={"max_tokens": 1000}
)

print(f"Processing time: {result['processing_time_ms']}ms")
print(f"Context window utilized: {result['context_advantage']}")
```

## üîÑ Backward Compatibility

The migration maintains backward compatibility:

- `EnhancedTinyLlamaAgent` still works (aliased to `EnhancedPhi3Agent`)
- All existing API endpoints remain functional
- Configuration changes are additive, not breaking

## üìä Performance Monitoring

### Key Metrics to Monitor

1. **Response Quality**: Check `quality_scores` in responses
2. **Context Usage**: Monitor `used_context` and `sources` count
3. **Processing Time**: Track `processing_time_ms` improvements
4. **Memory Usage**: Monitor system memory consumption

### Expected Improvements

- **Better Answers**: More coherent, contextually accurate responses
- **Longer Context**: Ability to process more retrieved documents
- **Faster Inference**: Especially on Apple Silicon
- **Reduced Hallucinations**: More reliable factual responses

## üõ†Ô∏è Troubleshooting

### Common Issues

#### 1. Model Download Fails

```bash
# Check disk space (need ~7GB)
df -h

# Check internet connection
ping huggingface.co

# Manual download
python -c "from transformers import AutoModelForCausalLM; AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-128k-instruct')"
```

#### 2. Memory Issues

```bash
# Check available memory
free -h  # Linux
vm_stat  # macOS

# Force CPU usage if needed
export FORCE_CPU_ONLY=true
```

#### 3. MPS (Apple Silicon) Issues

```bash
# If you encounter Metal shader errors
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

### Performance Optimization

#### For Apple Silicon (M1/M2)

```python
# Optimal settings for M1/M2
response_config = {
    "max_tokens": 500,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
}
```

#### For CPU-only Systems

```python
# Conservative settings for CPU
response_config = {
    "max_tokens": 300,
    "temperature": 0.8,
}
```

## üîê Security Considerations

- **Model Trust**: Phi-3 requires `trust_remote_code=True` (Microsoft model)
- **Cache Location**: Models stored in `~/.cache/huggingface/hub/`
- **Network Access**: Initial download requires internet connection

## üìà Next Steps

After successful migration:

1. **Restart Services**: Restart Django server and Celery workers
2. **Monitor Performance**: Check response quality and speed
3. **Optimize Settings**: Adjust `max_tokens` and `temperature` for your use case
4. **Consider Reprocessing**: Re-chunk knowledge base for optimal performance
5. **Update Documentation**: Update any internal docs referencing TinyLlama

## üÜò Support

If you encounter issues:

1. **Check Logs**: Look for error messages in Django logs
2. **Run Migration Script**: Use `python migration_to_phi3.py` for diagnostics
3. **Verify Requirements**: Ensure all dependencies are installed
4. **Memory Check**: Confirm sufficient RAM available

## üéâ Benefits Recap

After migration, you'll have:

- ‚úÖ **16x larger context window** for better RAG performance
- ‚úÖ **Optimized Apple Silicon support** for M1/M2 Macs
- ‚úÖ **Superior instruction following** and JSON generation
- ‚úÖ **Better knowledge integration** from longer documents
- ‚úÖ **Maintained backward compatibility** with existing code
- ‚úÖ **Enhanced response quality** with reduced hallucinations

Your Omeruta Brain is now powered by one of the best small language models available! üß†‚ú®
